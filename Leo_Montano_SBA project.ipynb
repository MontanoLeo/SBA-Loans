{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62834763",
   "metadata": {},
   "source": [
    "# Project \n",
    "\n",
    "Project is to practice Data Science concepts learned so far.\n",
    "\n",
    "The project will include following tasks:\n",
    "- Load dataset\n",
    "- Clean up the data:\n",
    "    - Encode replace missing values\n",
    "    - Replace features values that appear incorrect\n",
    "- Encode categorical variables\n",
    "- Split dataset to Train/Test/Validation\n",
    "- Add engineered features\n",
    "- Train and tune ML model\n",
    "- Provide final metrics using Validation dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341cb74",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "The dataset for Lab-2 is sample of the SBA dataset posted on Kaggle.\n",
    "The dataset is from the U.S. Small Business Administration (SBA) The U.S. SBA was founded in 1953 on the principle of promoting and assisting small enterprises in the U.S. credit market (SBA Overview and History, US Small Business Administration (2015)). Small businesses have been a primary source of job creation in the United States; therefore, fostering small business formation and growth has social benefits by creating job opportunities and reducing unemployment. There have been many success stories of start-ups receiving SBA loan guarantees such as FedEx and Apple Computer. However, there have also been stories of small businesses and/or start-ups that have defaulted on their SBA-guaranteed loans.  \n",
    "More info on the original dataset: https://www.kaggle.com/mirbektoktogaraev/should-this-loan-be-approved-or-denied\n",
    "\n",
    "**Don't use original dataset, use only dataset provided with project requirements in eLearning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9b618",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Use dataset provided in the eLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "\n",
    "#plots\n",
    "#!pip install plotnine\n",
    "import plotnine\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import bisect\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#models\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "\n",
    "#metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve \n",
    "from plotnine import *\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb889838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Mar 18 18:25:50 2019\n",
    "\n",
    "Purpose: Analyze input Pandas DataFrame and return stats per column\n",
    "Details: The function calculates levels for categorical variables and allows to analyze summarized information\n",
    "\n",
    "To view wide table set following Pandas options:\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth',200)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "def describe_more(df,normalize_ind=False, weight_column=None, skip_columns=[], dropna=True):\n",
    "    var = [] ; l = [] ; t = []; unq =[]; min_l = []; max_l = [];\n",
    "    assert isinstance(skip_columns, list), \"Argument skip_columns should be list\"\n",
    "    if weight_column is not None:\n",
    "        if weight_column not in list(df.columns):\n",
    "            raise AssertionError('weight_column is not a valid column name in the input DataFrame')\n",
    "      \n",
    "    for x in df:\n",
    "        if x in skip_columns:\n",
    "            pass\n",
    "        else:\n",
    "            var.append( x )\n",
    "            uniq_counts = len(pd.value_counts(df[x],dropna=dropna))\n",
    "            uniq_counts = len(pd.value_counts(df[x], dropna=dropna)[pd.value_counts(df[x],dropna=dropna)>0])\n",
    "            l.append(uniq_counts)\n",
    "            t.append( df[ x ].dtypes )\n",
    "            min_l.append(df[x].apply(str).str.len().min())\n",
    "            max_l.append(df[x].apply(str).str.len().max())\n",
    "            if weight_column is not None and x not in skip_columns:\n",
    "                df2 = df.groupby(x).agg({weight_column: 'sum'}).sort_values(weight_column, ascending=False)\n",
    "                df2['authtrans_vts_cnt']=((df2[weight_column])/df2[weight_column].sum()).round(2)\n",
    "                unq.append(df2.head(n=100).to_dict()[weight_column])\n",
    "            else:\n",
    "                df_cat_d = df[x].value_counts(normalize=normalize_ind,dropna=dropna).round(decimals=2)\n",
    "                df_cat_d = df_cat_d[df_cat_d>0]\n",
    "                #unq.append(df[x].value_counts().iloc[0:100].to_dict())\n",
    "                unq.append(df_cat_d.iloc[0:100].to_dict())\n",
    "            \n",
    "    levels = pd.DataFrame( { 'A_Variable' : var , 'Levels' : l , 'Datatype' : t ,\n",
    "                             'Min Length' : min_l,\n",
    "                             'Max Length': max_l,\n",
    "                             'Level_Values' : unq} )\n",
    "    #levels.sort_values( by = 'Levels' , inplace = True )\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2c254",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/monta/OneDrive/Desktop/BUAN_AML/Projects/SBA_loans_project_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b4aea",
   "metadata": {},
   "source": [
    "**Review dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f9eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc_df = describe_more(data)\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1af41",
   "metadata": {},
   "source": [
    "## Dataset preparation and clean-up\n",
    "\n",
    "Modify and clean-up the dataset as following:\n",
    "- Replace encode Na/Null values\n",
    "- Convert the strings styled as '$XXXX.XX' to float values. Columns = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv']\n",
    "- Convert MIS_Status to 0/1. Make value \"CHGOFF\" as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a0fd8",
   "metadata": {},
   "source": [
    "#### Here I consolidated the preprocessing into a function. This makes it easier to save and to also\n",
    "#### replicate the steps. I dropped columns with missing values. This way any missing values in a new dataset\n",
    "#### would intentionally count against the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c7ada",
   "metadata": {},
   "source": [
    "### Engineered features\n",
    "\n",
    "### 2 types of features, 2 new columns for each\n",
    "\n",
    "#### 1st new column: Existing_Business_Job \n",
    "#### 2nd new column: New_Business_Job\n",
    "New Exist(1 = Exist, 2 = New Business) : CreateJob This can give insight to expansion, or if creating jobs contributed to their ability to pay off.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_scoring(df):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \"\"\"\"\n",
    "    Function is to clean the dataset before target encoding and scoring\n",
    "    \n",
    "    FLOW: Create function to help convert\n",
    "    define col\n",
    "    drop nas\n",
    "    modify cols to make easier model scoring\n",
    "    \n",
    "    Then create Engineered features\n",
    "    \"\"\"\n",
    "    def convert3(x):\n",
    "        import pandas as pd\n",
    "        return pd.to_numeric(x,errors='ignore', downcast=\"float\")\n",
    "\n",
    "    cols_to_strip = ['DisbursementGross','BalanceGross','GrAppv','SBA_Appv']\n",
    "\n",
    "    df['MIS_Status'] = df['MIS_Status'].replace({'P I F': 0, 'CHGOFF': 1})\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    for col in cols_to_strip:\n",
    "        df[col] = df[col].str.replace('[$,]', '', regex=True)\n",
    "        df[col] = df[col].apply(convert3)\n",
    "\n",
    "    df = df[(df['RevLineCr'] == 'Y') | (df['RevLineCr'] == 'N')]\n",
    "    df = df[(df['LowDoc'] == 'Y') | (df['LowDoc'] == 'N')]\n",
    "    df = df.astype({'Zip': 'object', 'UrbanRural': 'object','NewExist':'int64'})\n",
    "    df= df[df.NewExist != 0]\n",
    "    df = df.astype({'NewExist':'object'})    \n",
    "    df= df[(df['NewExist'] == 1) | (df['NewExist'] == 2)]\n",
    "\n",
    "    ##Engineered Features------------------------------------\n",
    "    ## New Exist(1 = Exist, 2 = New Business) : CreateJob This can give insight to expansion, or simply trying to survive\n",
    "    def myfunc(NewExist, CreateJob):\n",
    "        if NewExist==2 and CreateJob!=0:\n",
    "            Existing_Business_Job = 1\n",
    "        else:\n",
    "            Existing_Business_Job = 0\n",
    "        return Existing_Business_Job\n",
    "    df['Existing_Business_Job'] = df.apply(lambda x: myfunc(x['NewExist'], x['CreateJob']), axis=1)\n",
    "\n",
    "    def myfunc(NewExist, CreateJob):\n",
    "        if NewExist==1 and CreateJob!=0:\n",
    "            New_Business_Job = 1\n",
    "        else:\n",
    "            New_Business_Job = 0\n",
    "        return New_Business_Job\n",
    "    df['New_Business_Job'] = df.apply(lambda x: myfunc(x['NewExist'], x['CreateJob']), axis=1)\n",
    "    ## Term( loan term in months) : RevLineCr\n",
    "    ## For those with short or long term amounts, if used to see if they defaulted, \n",
    "    #we can see predict if a revLineCr does more harm than good.\n",
    "    ## Because revolving line of credit empowers the borrower, i am curious to see \n",
    "    #if it can help probability of defaulting\n",
    "    def myfunc(Term, RevLineCr):\n",
    "        if Term>=96 and RevLineCr == 'Y':\n",
    "            Long_Term_RevCr = 1\n",
    "        else:\n",
    "            Long_Term_RevCr = 0\n",
    "        return Long_Term_RevCr\n",
    "    df['Long_Term_RevCr'] = df.apply(lambda x: myfunc(x['Term'], x['RevLineCr']), axis=1)\n",
    "\n",
    "    def myfunc(Term, RevLineCr):\n",
    "        if Term <96 and RevLineCr == 'Y':\n",
    "            Short_Term_RevCr = 1\n",
    "        else:\n",
    "            Short_Term_RevCr = 0\n",
    "        return Short_Term_RevCr\n",
    "    df['Short_Term_RevCr'] = df.apply(lambda x: myfunc(x['Term'], x['RevLineCr']), axis=1)\n",
    "    \n",
    "    ###----------------------Dummies/OHE\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    BalanceGross_dummies = pd.get_dummies(df.BalanceGross,prefix='BalanceGross')\n",
    "    df = pd.concat([df, BalanceGross_dummies], axis=1)\n",
    "\n",
    "    LowDoc_dummies = pd.get_dummies(df.LowDoc,prefix='LowDoc')\n",
    "    df = pd.concat([df, LowDoc_dummies], axis=1)\n",
    "    \n",
    "    UrbanRural_dummies = pd.get_dummies(df.UrbanRural,prefix='UrbanRural')\n",
    "    df = pd.concat([df, UrbanRural_dummies], axis=1)\n",
    "\n",
    "\n",
    "    #OHE/pandas dummies\n",
    "    df.drop('LowDoc', axis=1, inplace=True)\n",
    "    df.drop('UrbanRural', axis=1, inplace=True)\n",
    "    df.drop('BalanceGross', axis=1, inplace=True)\n",
    "    ## used to make features\n",
    "    df.drop('Term', axis=1, inplace=True)\n",
    "    df.drop('NewExist', axis=1, inplace=True)\n",
    "    df.drop('RevLineCr', axis=1, inplace=True)\n",
    "    df.drop('CreateJob', axis=1, inplace=True)\n",
    "\n",
    "    return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_for_scoring(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f21dcf",
   "metadata": {},
   "source": [
    "## Categorical variables encoding\n",
    "\n",
    "Encode categorical variables using either one of the techniques below. Don't use LabelEncoder.\n",
    "- One-hot-encoder for variables with less than 10 valid values. Name your new columns \"Original_name\"_valid_value\n",
    "- (If using sklearn) Target encoder from the following library: https://contrib.scikit-learn.org/category_encoders/index.html . Name your new column \"Original_name\"_trg\n",
    "- (If using H2O) Use H2O target encoder\n",
    "\n",
    "\n",
    "Example of use for target encoder:\n",
    "```\n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.TargetEncoder(cols=[...])\n",
    "\n",
    "encoder.fit(X, y)\n",
    "X_cleaned = encoder.transform(X_dirty)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb05146",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    obj= {}\n",
    "    if df[col].dtype == 'object':\n",
    "        obj[col] = df[col].nunique()\n",
    "        for key, value in obj.items():\n",
    "            print('obj_col',key,': ', value)\n",
    "print()\n",
    "for col in df.columns:\n",
    "    integer={}\n",
    "    if df[col].dtype == 'int64':\n",
    "        integer[col] = df[col].nunique()\n",
    "        for key, value in integer.items():\n",
    "            print('int_col',key,': ', value)\n",
    "print()\n",
    "for col in df.columns:\n",
    "    float_data={}\n",
    "    if df[col].dtype == 'float32':\n",
    "        float_data[col] = df[col].nunique()\n",
    "        for key, value in float_data.items():\n",
    "            print('float_col',key,': ', value)\n",
    "            \n",
    "print()\n",
    "for col in df.columns:\n",
    "    float_data={}\n",
    "    if df[col].dtype == 'float64':\n",
    "        float_data[col] = df[col].nunique()\n",
    "        for key, value in float_data.items():\n",
    "            print('float_col',key,': ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11546350",
   "metadata": {},
   "source": [
    "### Due to dropping the columns that were used to create new features, I placed that in the function above. \n",
    "### This changed those that needed to be target encoded and scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32455814",
   "metadata": {},
   "source": [
    "## Target Encoding via sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7411200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop(columns = ['MIS_Status'], axis = 1).copy()\n",
    "y = df[['MIS_Status']]\n",
    "te = ce.TargetEncoder()\n",
    "\n",
    "# Splitting into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                    y,\n",
    "                                                    test_size = .30,\n",
    "                                                    train_size = .7,\n",
    "                                                    random_state=9999)\n",
    "suffix = 'trg'\n",
    "cols_te =['City','Zip','State','Bank','BankState']\n",
    "\n",
    "x_train[cols_te] = te.fit_transform(x_train[cols_te], y_train)\n",
    "x_test[cols_te] = te.transform(x_test[cols_te], y_test)\n",
    "\n",
    "x_train.columns= x_train.columns.map(lambda x : x +'_trg' if x in cols_te else x)\n",
    "x_test.columns = x_test.columns.map(lambda x : x +'_trg' if x in cols_te else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9add5b1",
   "metadata": {},
   "source": [
    "## MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490781b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_scale = ['NAICS','NoEmp','RetainedJob','FranchiseCode','DisbursementGross','GrAppv','SBA_Appv']\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "suffix = '_sc'\n",
    "\n",
    "x_train[cols_to_scale] = sc.fit_transform(x_train[cols_to_scale])\n",
    "x_test[cols_to_scale] = sc.transform(x_test[cols_to_scale])\n",
    "\n",
    "x_train.columns= x_train.columns.map(lambda x : x +'_sc' if x in cols_to_scale else x)\n",
    "x_test.columns = x_test.columns.map(lambda x : x +'_sc' if x in cols_to_scale else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0bfa4",
   "metadata": {},
   "source": [
    "# Model Training / ## Model Tuning\n",
    "\n",
    "Depending on the model of your choice, you might need to use appropriate scaler for numerical variables.\n",
    "\n",
    "Train at least two types of models from the below list.\n",
    "If you use sklearn libraries:\n",
    "- Logistic regression\n",
    "- SVM\n",
    "- Decision Tree\n",
    "\n",
    "If you use H2O libraries:\n",
    "- GLM\n",
    "- SVM\n",
    "- NaÃ¯ve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced690c6",
   "metadata": {},
   "source": [
    "Choose one model from the above list. You should provide reasoning on why you have picked the model over others. Perform tuning for the selected model:\n",
    "- Hyper-parameter tuning. Your hyper-parameter search space should have at least 50 combinations.\n",
    "- To avoid overfitting and provide you with reasonable estimate of model performance on hold-out dataset, you will need to split your dataset as following:\n",
    "    - Train, will be used to train model\n",
    "    - Validation, will be used to validate model each round of training\n",
    "    - Testing, will be used to provide final performance metrics, used only once on the final model\n",
    "- Feature engineering. You should add at least two engineered features.  For example, add feature which is combination of two features.\n",
    "- If your model returns probability, calculate probability threshold to maximize F1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eea8b0",
   "metadata": {},
   "source": [
    "### SVC Model. Did the hyper parameter tuning sooner to give a cleaner flow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c1dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train_svm = x_train.copy()\n",
    "Y_train_svm = y_train.copy()\n",
    "\n",
    "X_test_svm= x_test.copy()\n",
    "Y_test_svm = y_test.copy()\n",
    "\n",
    "# ## We set the parameters to use for the best estimator. \n",
    "# svm = SVC(kernel='poly', max_iter=700, verbose=False)\n",
    "# degree = [2,3,4,5,7]\n",
    "# parameters = dict(degree = degree)\n",
    "\n",
    "# ##utilize the best estimator to find the best degree\n",
    "# GridSearch = GridSearchCV(svm, parameters)\n",
    "# Fitted_Model = GridSearch.fit(X_train_svm, Y_train_svm)\n",
    "# print(Fitted_Model.best_estimator_.get_params()['degree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(kernel='poly', degree=4, max_iter=700, verbose=False)\n",
    "\n",
    "SVM.fit(X_train_svm,Y_train_svm)\n",
    "SVM_y_pred = SVM.predict(X_test_svm)\n",
    "\n",
    "labels = np.unique(Y_test_svm)\n",
    "confusion = metrics.confusion_matrix(Y_test_svm, SVM_y_pred, labels = np.unique(Y_test_svm))\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(confusion, index=labels, columns=labels))\n",
    "print()\n",
    "print('Classification Report')\n",
    "print(classification_report(Y_test_svm, SVM_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b9f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Target Column:\", y_train.sum()/len(y_train)) ## unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7bca9",
   "metadata": {},
   "source": [
    "### Decision Tree - chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb4648b",
   "metadata": {},
   "source": [
    "### I decided chose the DT model due to the poor scoring for the SVM.\n",
    "### The poor scoring may be due to the imbalance of the dataset. \n",
    "### Also, the preprocssing could have impacted the model to make the classification less accurate. \n",
    "### The Dt will deal better with the collinearity overall due to some features being related. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_dt = x_train.copy()\n",
    "Y_train_dt = y_train.copy()\n",
    "\n",
    "X_test_dt = x_test.copy()\n",
    "Y_test_dt = y_test.copy()\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "pipe = Pipeline(steps=[('dtc', dtc)])\n",
    "\n",
    "max_depth = [2,4,6,8,10,12,100]\n",
    "ccp_alpha = [0,0.0005, 0.001, 0.0015, 0.002, 0.0025]\n",
    "criterion = ['entropy']\n",
    "\n",
    "parameters = dict(dtc__max_depth=max_depth,\n",
    "                  dtc__ccp_alpha =ccp_alpha,\n",
    "                  dtc__criterion=criterion)\n",
    "\n",
    "dtc = GridSearchCV(dtc, parameters)\n",
    "GridSearch = GridSearchCV(pipe, parameters)\n",
    "DTC_Model = GridSearch.fit(X_train_dt, Y_train_dt)\n",
    "\n",
    "print('Best max_depth:', DTC_Model.best_estimator_.get_params()['dtc__max_depth'])\n",
    "print('Best ccp_alpha:', DTC_Model.best_estimator_.get_params()['dtc__ccp_alpha'])\n",
    "print('Best Criterion:', DTC_Model.best_estimator_.get_params()['dtc__criterion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dt2 = x_train.copy()\n",
    "Y_train_dt2 = y_train.copy()\n",
    "\n",
    "X_test_dt2 = x_test.copy()\n",
    "Y_test_dt2 = y_test.copy()\n",
    "\n",
    "dtc3 = DecisionTreeClassifier(random_state =0, max_depth =12, ccp_alpha= 0, criterion ='entropy')\n",
    "dtc3.fit(X_train_dt2,Y_train_dt2)\n",
    "pred_train_dt = dtc3.predict(X_train_dt2)\n",
    "pred_test_dt = dtc3.predict(X_test_dt2)\n",
    "\n",
    "##We see that the weighted f1score is \n",
    "labels = np.unique(Y_test_dt2)\n",
    "\n",
    "##Train\n",
    "print('Decision Tree Model 2')\n",
    "confusion = metrics.confusion_matrix(Y_train_dt2, pred_train_dt, labels = np.unique(Y_test_dt2))\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(confusion, index=labels, columns=labels))\n",
    "print(classification_report(Y_train_dt2, pred_train_dt))\n",
    "print()\n",
    "\n",
    "##Test\n",
    "print('Decision Tree Model 2')\n",
    "confusion = metrics.confusion_matrix(Y_test_dt2, pred_test_dt, labels = np.unique(Y_test_dt2))\n",
    "print('Confusion matrix')\n",
    "print(pd.DataFrame(confusion, index=labels, columns=labels))\n",
    "print(classification_report(Y_test_dt2, pred_test_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc381c",
   "metadata": {},
   "source": [
    "## Save all artifacts\n",
    "\n",
    "Save all artifacts needed for scoring function:\n",
    "- Trained model\n",
    "- Encoders\n",
    "\n",
    "You should restart your Kernel now to properly test scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1fba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cloudpickle\n",
    "import pickle\n",
    "import cloudpickle\n",
    "from pickle import dump\n",
    "\n",
    "with open('preprocessing_for_scoring', 'wb') as preprocessing_for_scoring_file:\n",
    "    cloudpickle.dump(preprocessing_for_scoring, preprocessing_for_scoring_file)\n",
    "\n",
    "#Saving the MinMax Scaler\n",
    "dump(te, open('te.pkl', 'wb'))\n",
    "with open('te', 'wb') as te_file:\n",
    "    cloudpickle.dump(te, te_file)\n",
    "    \n",
    "#Saving the MinMax Scaler\n",
    "dump(sc, open('sc.pkl', 'wb'))\n",
    "with open('sc', 'wb') as sc_file:\n",
    "    cloudpickle.dump(sc, sc_file)\n",
    "\n",
    "#Saving the Decision Tree\n",
    "dump(dtc3, open('dtc3.pkl', 'wb'))\n",
    "with open('dtc3', 'wb') as dtc3_file:\n",
    "    cloudpickle.dump(dtc3, dtc3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e23e48",
   "metadata": {},
   "source": [
    "# -----------------Restart Kernel Here---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2e830",
   "metadata": {},
   "source": [
    "## Model Scoring\n",
    "\n",
    "Write function that will load artifacts from above, transform and score on a new dataset.\n",
    "Your function should return Python list of labels. For example: [0,1,0,1,1,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774cfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_scorer(df):\n",
    "    import pandas as pd\n",
    "    import cloudpickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from pickle import dump\n",
    "    from pickle import load\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import precision_recall_curve \n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "#     \"\"\"\n",
    "#     Function to score input dataset.\n",
    "    \n",
    "#     Input: dataset in Pandas DataFrame format\n",
    "#     Output: Python list of labels in the same order as input records\n",
    "    \n",
    "#     Flow:\n",
    "        \n",
    "#         - Load Df_cleaning_Preprocessor\n",
    "#         - load target encoder\n",
    "#         - Transform dataset\n",
    "#         - load model\n",
    "#         - Score dataset\n",
    "#         - Return labels\n",
    "#     \"\"\"\n",
    "    cols_to_scale = ['NAICS','NoEmp','RetainedJob','FranchiseCode','DisbursementGross','GrAppv','SBA_Appv']\n",
    "    cols_te =['City','Zip','State','Bank','BankState']\n",
    "    \n",
    "    print('Cleaning Df....')\n",
    "    with open(r\"preprocessing_for_scoring\", \"rb\") as preprocessing_for_scoring_file:\n",
    "        preprocessing_for_scoring = cloudpickle.load(preprocessing_for_scoring_file)\n",
    "    df = preprocessing_for_scoring(df)\n",
    "    preprocessing_for_scoring_file.close()\n",
    "    \n",
    "    print('Target Encoding.....')\n",
    "    with open(r\"te\", \"rb\") as te_file:\n",
    "        te = cloudpickle.load(te_file)\n",
    "    df[cols_te] = te.transform(df[cols_te])\n",
    "    df.columns= df.columns.map(lambda x : x +'_trg' if x in cols_te else x)\n",
    "\n",
    "    \n",
    "    print('Scaling columns....')\n",
    "    scaler = load(open('sc.pkl', 'rb'))\n",
    "    with open(r\"sc\", \"rb\") as sc_file:\n",
    "        sc = cloudpickle.load(sc_file)\n",
    "    df[cols_to_scale] = sc.transform(df[cols_to_scale])\n",
    "    df.columns= df.columns.map(lambda x : x +'_sc' if x in cols_to_scale else x)\n",
    "    \n",
    "    print('Loading Decision tree model....')\n",
    "    decision_tree = load(open('dtc3.pkl', 'rb'))\n",
    "    with open(r\"dtc3\", \"rb\") as dtc3_file:\n",
    "        dtc3 = cloudpickle.load(dtc3_file)\n",
    "    \n",
    "    Y = df['MIS_Status']\n",
    "    X = df.drop(['MIS_Status'], axis = 1, inplace = True)\n",
    "    pred_dt = dtc3.predict(df)\n",
    "    labels = np.unique(Y)\n",
    "\n",
    "    \n",
    "    print('Scoring your df ......')\n",
    "    Confusion_Matrix = metrics.confusion_matrix(Y, pred_dt, labels = np.unique(Y))\n",
    "    print('Decision Tree Model')\n",
    "    print(Confusion_Matrix,pd.DataFrame(Confusion_Matrix, index=labels, columns=labels))\n",
    "    print('Classification Report:')\n",
    "    return print(classification_report(Y,pred_dt,labels= np.unique(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a4f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Df....\n",
      "Target Encoding.....\n",
      "Scaling columns....\n",
      "Loading Decision tree model....\n",
      "Scoring your df ......\n",
      "Decision Tree Model\n",
      "[[431000  20078]\n",
      " [ 59738  40074]]         0.0    1.0\n",
      "0.0  431000  20078\n",
      "1.0   59738  40074\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.96      0.92    451078\n",
      "         1.0       0.67      0.40      0.50     99812\n",
      "\n",
      "    accuracy                           0.86    550890\n",
      "   macro avg       0.77      0.68      0.71    550890\n",
      "weighted avg       0.84      0.86      0.84    550890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('C:/Users/monta/OneDrive/Desktop/BUAN_AML/Projects/SBA_loans_project_1.csv')\n",
    "final_scorer(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
